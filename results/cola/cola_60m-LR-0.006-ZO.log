W0502 23:05:00.514000 796712 torch/distributed/run.py:792] 
W0502 23:05:00.514000 796712 torch/distributed/run.py:792] *****************************************
W0502 23:05:00.514000 796712 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0502 23:05:00.514000 796712 torch/distributed/run.py:792] *****************************************
Starting script
2025-05-02 23:05:06.390 | INFO     | __main__:main:238 - Global rank 0, local rank 0, device: 0
2025-05-02 23:05:06.396 | INFO     | __main__:main:249 - Process group initialized
2025-05-02 23:05:06.396 | INFO     | __main__:main:261 - 10-2-6400-320
Starting script
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
2025-05-02 23:05:06.472 | INFO     | __main__:main:238 - Global rank 1, local rank 1, device: 1
2025-05-02 23:05:06.479 | INFO     | __main__:main:249 - Process group initialized
wandb: Currently logged in as: yequan_zhao (yequan_zhao-university-of-california-santa-barbara) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.9
wandb: Run data is saved locally in /workspace/wandb/run-20250502_230506-t8t4a4as
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cola_60m-LR-0.006-ZO
wandb: ‚≠êÔ∏è View project at https://wandb.ai/yequan_zhao-university-of-california-santa-barbara/cola
wandb: üöÄ View run at https://wandb.ai/yequan_zhao-university-of-california-santa-barbara/cola/runs/t8t4a4as
2025-05-02 23:05:07.358 | INFO     | __main__:main:287 - Using dist with rank 0 (only rank 0 will log)
2025-05-02 23:05:07.358 | INFO     | __main__:main:288 - ****************************************
2025-05-02 23:05:07.358 | INFO     | __main__:main:289 - Starting training with the arguments
2025-05-02 23:05:07.358 | INFO     | __main__:main:291 - model_type                     cola
2025-05-02 23:05:07.358 | INFO     | __main__:main:291 - run_name                       cola_60m-LR-0.006-ZO
2025-05-02 23:05:07.358 | INFO     | __main__:main:291 - wandb_project                  cola
2025-05-02 23:05:07.358 | INFO     | __main__:main:291 - model_config                   cola_configs/cola_60m.json
2025-05-02 23:05:07.358 | INFO     | __main__:main:291 - offline_mode                   False
2025-05-02 23:05:07.358 | INFO     | __main__:main:291 - continue_from                  None
2025-05-02 23:05:07.358 | INFO     | __main__:main:291 - batch_size                     320
2025-05-02 23:05:07.359 | INFO     | __main__:main:291 - gradient_accumulation          10
2025-05-02 23:05:07.359 | INFO     | __main__:main:291 - total_batch_size               6400
2025-05-02 23:05:07.359 | INFO     | __main__:main:291 - max_length                     256
2025-05-02 23:05:07.359 | INFO     | __main__:main:291 - optimizer                      adamw
2025-05-02 23:05:07.359 | INFO     | __main__:main:291 - lr                             0.006
2025-05-02 23:05:07.359 | INFO     | __main__:main:291 - scheduler                      cosine
2025-05-02 23:05:07.359 | INFO     | __main__:main:291 - min_lr_ratio                   0.1
2025-05-02 23:05:07.359 | INFO     | __main__:main:291 - activation_checkpointing       False
2025-05-02 23:05:07.359 | INFO     | __main__:main:291 - weight_decay                   0.01
2025-05-02 23:05:07.359 | INFO     | __main__:main:291 - warmup_steps                   2000
2025-05-02 23:05:07.359 | INFO     | __main__:main:291 - eval_every                     1000
2025-05-02 23:05:07.359 | INFO     | __main__:main:291 - num_training_steps             10000
2025-05-02 23:05:07.359 | INFO     | __main__:main:291 - max_train_tokens               None
2025-05-02 23:05:07.359 | INFO     | __main__:main:291 - save_every                     10000
2025-05-02 23:05:07.359 | INFO     | __main__:main:291 - save_dir                       checkpoints/cola_60m-2025-05-02-23-05-06
2025-05-02 23:05:07.359 | INFO     | __main__:main:291 - tags                           None
2025-05-02 23:05:07.359 | INFO     | __main__:main:291 - dtype                          bfloat16
2025-05-02 23:05:07.359 | INFO     | __main__:main:291 - workers                        8
2025-05-02 23:05:07.359 | INFO     | __main__:main:291 - seed                           42
2025-05-02 23:05:07.360 | INFO     | __main__:main:291 - name                           test
2025-05-02 23:05:07.360 | INFO     | __main__:main:291 - grad_clipping                  0.5
2025-05-02 23:05:07.360 | INFO     | __main__:main:291 - beta1                          0.0
2025-05-02 23:05:07.360 | INFO     | __main__:main:291 - single_gpu                     False
2025-05-02 23:05:07.360 | INFO     | __main__:main:291 - ZO_Estim                       True
2025-05-02 23:05:07.360 | INFO     | __main__:main:292 - ****************************************
layer model.layers.0.self_attn.q_proj
layer model.layers.0.self_attn.k_proj
layer model.layers.0.self_attn.v_proj
layer model.layers.0.self_attn.o_proj
layer model.layers.0.mlp.gate_proj
layer model.layers.0.mlp.up_proj
layer model.layers.0.mlp.down_proj
layer model.layers.1.self_attn.q_proj
layer model.layers.1.self_attn.k_proj
layer model.layers.1.self_attn.v_proj
layer model.layers.1.self_attn.o_proj
layer model.layers.1.mlp.gate_proj
layer model.layers.1.mlp.up_proj
layer model.layers.1.mlp.down_proj
layer model.layers.2.self_attn.q_proj
layer model.layers.2.self_attn.k_proj
layer model.layers.2.self_attn.v_proj
layer model.layers.2.self_attn.o_proj
layer model.layers.2.mlp.gate_proj
layer model.layers.2.mlp.up_proj
layer model.layers.2.mlp.down_proj
layer model.layers.3.self_attn.q_proj
layer model.layers.3.self_attn.k_proj
layer model.layers.3.self_attn.v_proj
layer model.layers.3.self_attn.o_proj
layer model.layers.3.mlp.gate_proj
layer model.layers.3.mlp.up_proj
layer model.layers.3.mlp.down_proj
layer model.layers.4.self_attn.q_proj
layer model.layers.4.self_attn.k_proj
layer model.layers.4.self_attn.v_proj
layer model.layers.4.self_attn.o_proj
layer model.layers.4.mlp.gate_proj
layer model.layers.4.mlp.up_proj
layer model.layers.4.mlp.down_proj
layer model.layers.5.self_attn.q_proj
layer model.layers.5.self_attn.k_proj
layer model.layers.5.self_attn.v_proj
layer model.layers.5.self_attn.o_proj
layer model.layers.5.mlp.gate_proj
layer model.layers.5.mlp.up_proj
layer model.layers.5.mlp.down_proj
layer model.layers.6.self_attn.q_proj
layer model.layers.6.self_attn.k_proj
layer model.layers.6.self_attn.v_proj
layer model.layers.6.self_attn.o_proj
layer model.layers.6.mlp.gate_proj
layer model.layers.6.mlp.up_proj
layer model.layers.6.mlp.down_proj
layer model.layers.7.self_attn.q_proj
layer model.layers.7.self_attn.k_proj
layer model.layers.7.self_attn.v_proj
layer model.layers.7.self_attn.o_proj
layer model.layers.7.mlp.gate_proj
layer model.layers.7.mlp.up_proj
layer model.layers.7.mlp.down_proj
2025-05-02 23:05:15.891 | INFO     | __main__:main:302 - Shuffling data with seed 42
2025-05-02 23:05:16.528 | WARNING  | __main__:main:390 - Did not find training state in None, global step will start from zero
2025-05-02 23:05:16.529 | INFO     | __main__:main:393 - ****************************************
layer model.layers.0.self_attn.q_proj
layer model.layers.0.self_attn.k_proj
layer model.layers.0.self_attn.v_proj
layer model.layers.0.self_attn.o_proj
layer model.layers.0.mlp.gate_proj
layer model.layers.0.mlp.up_proj
layer model.layers.0.mlp.down_proj
layer model.layers.1.self_attn.q_proj
layer model.layers.1.self_attn.k_proj
layer model.layers.1.self_attn.v_proj
layer model.layers.1.self_attn.o_proj
layer model.layers.1.mlp.gate_proj
layer model.layers.1.mlp.up_proj
layer model.layers.1.mlp.down_proj
layer model.layers.2.self_attn.q_proj
layer model.layers.2.self_attn.k_proj
layer model.layers.2.self_attn.v_proj
layer model.layers.2.self_attn.o_proj
layer model.layers.2.mlp.gate_proj
layer model.layers.2.mlp.up_proj
layer model.layers.2.mlp.down_proj
layer model.layers.3.self_attn.q_proj
layer model.layers.3.self_attn.k_proj
layer model.layers.3.self_attn.v_proj
layer model.layers.3.self_attn.o_proj
layer model.layers.3.mlp.gate_proj
layer model.layers.3.mlp.up_proj
layer model.layers.3.mlp.down_proj
layer model.layers.4.self_attn.q_proj
layer model.layers.4.self_attn.k_proj
layer model.layers.4.self_attn.v_proj
layer model.layers.4.self_attn.o_proj
layer model.layers.4.mlp.gate_proj
layer model.layers.4.mlp.up_proj
layer model.layers.4.mlp.down_proj
layer model.layers.5.self_attn.q_proj
layer model.layers.5.self_attn.k_proj
layer model.layers.5.self_attn.v_proj
layer model.layers.5.self_attn.o_proj
layer model.layers.5.mlp.gate_proj
layer model.layers.5.mlp.up_proj
layer model.layers.5.mlp.down_proj
layer model.layers.6.self_attn.q_proj
layer model.layers.6.self_attn.k_proj
layer model.layers.6.self_attn.v_proj
layer model.layers.6.self_attn.o_proj
layer model.layers.6.mlp.gate_proj
layer model.layers.6.mlp.up_proj
layer model.layers.6.mlp.down_proj
layer model.layers.7.self_attn.q_proj
layer model.layers.7.self_attn.k_proj
layer model.layers.7.self_attn.v_proj
layer model.layers.7.self_attn.o_proj
layer model.layers.7.mlp.gate_proj
layer model.layers.7.mlp.up_proj
layer model.layers.7.mlp.down_proj
2025-05-02 23:05:17.116 | INFO     | __main__:main:499 - Running with cola

2025-05-02 23:05:17.117 | INFO     | __main__:main:500 - 
ColaForCausalLM(
  (model): ColaModel(
    (embed_tokens): Embedding(32000, 512, padding_idx=0)
    (layers): ModuleList(
      (0-7): 8 x ColaDecoderLayer(
        (self_attn): ColaSdpaAttention(
          (q_proj): ColaLayer(
            cola_a: torch.Size([512, 128]), cola_b: torch.Size([128, 512]), bias: False
            (lr_act): SiLU()
          )
          (k_proj): ColaLayer(
            cola_a: torch.Size([512, 128]), cola_b: torch.Size([128, 512]), bias: False
            (lr_act): SiLU()
          )
          (v_proj): ColaLayer(
            cola_a: torch.Size([512, 128]), cola_b: torch.Size([128, 512]), bias: False
            (lr_act): SiLU()
          )
          (o_proj): ColaLayer(
            cola_a: torch.Size([512, 128]), cola_b: torch.Size([128, 512]), bias: False
            (lr_act): SiLU()
          )
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): ColaMLP(
          (gate_proj): ColaLayer(
            cola_a: torch.Size([512, 128]), cola_b: torch.Size([128, 1376]), bias: False
            (lr_act): SiLU()
          )
          (up_proj): ColaLayer(
            cola_a: torch.Size([512, 128]), cola_b: torch.Size([128, 1376]), bias: False
            (lr_act): SiLU()
          )
          (down_proj): ColaLayer(
            cola_a: torch.Size([1376, 128]), cola_b: torch.Size([128, 512]), bias: False
            (lr_act): SiLU()
          )
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((512,), eps=1e-06)
        (post_attention_layernorm): LlamaRMSNorm((512,), eps=1e-06)
      )
    )
    (norm): LlamaRMSNorm((512,), eps=1e-06)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=512, out_features=32000, bias=False)
)

2025-05-02 23:05:17.117 | INFO     | __main__:main:501 - All params: 
['model.embed_tokens.weight', 'model.layers.0.self_attn.q_proj.cola_a', 'model.layers.0.self_attn.q_proj.cola_b', 'model.layers.0.self_attn.k_proj.cola_a', 'model.layers.0.self_attn.k_proj.cola_b', 'model.layers.0.self_attn.v_proj.cola_a', 'model.layers.0.self_attn.v_proj.cola_b', 'model.layers.0.self_attn.o_proj.cola_a', 'model.layers.0.self_attn.o_proj.cola_b', 'model.layers.0.mlp.gate_proj.cola_a', 'model.layers.0.mlp.gate_proj.cola_b', 'model.layers.0.mlp.up_proj.cola_a', 'model.layers.0.mlp.up_proj.cola_b', 'model.layers.0.mlp.down_proj.cola_a', 'model.layers.0.mlp.down_proj.cola_b', 'model.layers.0.input_layernorm.weight', 'model.layers.0.post_attention_layernorm.weight', 'model.layers.1.self_attn.q_proj.cola_a', 'model.layers.1.self_attn.q_proj.cola_b', 'model.layers.1.self_attn.k_proj.cola_a', 'model.layers.1.self_attn.k_proj.cola_b', 'model.layers.1.self_attn.v_proj.cola_a', 'model.layers.1.self_attn.v_proj.cola_b', 'model.layers.1.self_attn.o_proj.cola_a', 'model.layers.1.self_attn.o_proj.cola_b', 'model.layers.1.mlp.gate_proj.cola_a', 'model.layers.1.mlp.gate_proj.cola_b', 'model.layers.1.mlp.up_proj.cola_a', 'model.layers.1.mlp.up_proj.cola_b', 'model.layers.1.mlp.down_proj.cola_a', 'model.layers.1.mlp.down_proj.cola_b', 'model.layers.1.input_layernorm.weight', 'model.layers.1.post_attention_layernorm.weight', 'model.layers.2.self_attn.q_proj.cola_a', 'model.layers.2.self_attn.q_proj.cola_b', 'model.layers.2.self_attn.k_proj.cola_a', 'model.layers.2.self_attn.k_proj.cola_b', 'model.layers.2.self_attn.v_proj.cola_a', 'model.layers.2.self_attn.v_proj.cola_b', 'model.layers.2.self_attn.o_proj.cola_a', 'model.layers.2.self_attn.o_proj.cola_b', 'model.layers.2.mlp.gate_proj.cola_a', 'model.layers.2.mlp.gate_proj.cola_b', 'model.layers.2.mlp.up_proj.cola_a', 'model.layers.2.mlp.up_proj.cola_b', 'model.layers.2.mlp.down_proj.cola_a', 'model.layers.2.mlp.down_proj.cola_b', 'model.layers.2.input_layernorm.weight', 'model.layers.2.post_attention_layernorm.weight', 'model.layers.3.self_attn.q_proj.cola_a', 'model.layers.3.self_attn.q_proj.cola_b', 'model.layers.3.self_attn.k_proj.cola_a', 'model.layers.3.self_attn.k_proj.cola_b', 'model.layers.3.self_attn.v_proj.cola_a', 'model.layers.3.self_attn.v_proj.cola_b', 'model.layers.3.self_attn.o_proj.cola_a', 'model.layers.3.self_attn.o_proj.cola_b', 'model.layers.3.mlp.gate_proj.cola_a', 'model.layers.3.mlp.gate_proj.cola_b', 'model.layers.3.mlp.up_proj.cola_a', 'model.layers.3.mlp.up_proj.cola_b', 'model.layers.3.mlp.down_proj.cola_a', 'model.layers.3.mlp.down_proj.cola_b', 'model.layers.3.input_layernorm.weight', 'model.layers.3.post_attention_layernorm.weight', 'model.layers.4.self_attn.q_proj.cola_a', 'model.layers.4.self_attn.q_proj.cola_b', 'model.layers.4.self_attn.k_proj.cola_a', 'model.layers.4.self_attn.k_proj.cola_b', 'model.layers.4.self_attn.v_proj.cola_a', 'model.layers.4.self_attn.v_proj.cola_b', 'model.layers.4.self_attn.o_proj.cola_a', 'model.layers.4.self_attn.o_proj.cola_b', 'model.layers.4.mlp.gate_proj.cola_a', 'model.layers.4.mlp.gate_proj.cola_b', 'model.layers.4.mlp.up_proj.cola_a', 'model.layers.4.mlp.up_proj.cola_b', 'model.layers.4.mlp.down_proj.cola_a', 'model.layers.4.mlp.down_proj.cola_b', 'model.layers.4.input_layernorm.weight', 'model.layers.4.post_attention_layernorm.weight', 'model.layers.5.self_attn.q_proj.cola_a', 'model.layers.5.self_attn.q_proj.cola_b', 'model.layers.5.self_attn.k_proj.cola_a', 'model.layers.5.self_attn.k_proj.cola_b', 'model.layers.5.self_attn.v_proj.cola_a', 'model.layers.5.self_attn.v_proj.cola_b', 'model.layers.5.self_attn.o_proj.cola_a', 'model.layers.5.self_attn.o_proj.cola_b', 'model.layers.5.mlp.gate_proj.cola_a', 'model.layers.5.mlp.gate_proj.cola_b', 'model.layers.5.mlp.up_proj.cola_a', 'model.layers.5.mlp.up_proj.cola_b', 'model.layers.5.mlp.down_proj.cola_a', 'model.layers.5.mlp.down_proj.cola_b', 'model.layers.5.input_layernorm.weight', 'model.layers.5.post_attention_layernorm.weight', 'model.layers.6.self_attn.q_proj.cola_a', 'model.layers.6.self_attn.q_proj.cola_b', 'model.layers.6.self_attn.k_proj.cola_a', 'model.layers.6.self_attn.k_proj.cola_b', 'model.layers.6.self_attn.v_proj.cola_a', 'model.layers.6.self_attn.v_proj.cola_b', 'model.layers.6.self_attn.o_proj.cola_a', 'model.layers.6.self_attn.o_proj.cola_b', 'model.layers.6.mlp.gate_proj.cola_a', 'model.layers.6.mlp.gate_proj.cola_b', 'model.layers.6.mlp.up_proj.cola_a', 'model.layers.6.mlp.up_proj.cola_b', 'model.layers.6.mlp.down_proj.cola_a', 'model.layers.6.mlp.down_proj.cola_b', 'model.layers.6.input_layernorm.weight', 'model.layers.6.post_attention_layernorm.weight', 'model.layers.7.self_attn.q_proj.cola_a', 'model.layers.7.self_attn.q_proj.cola_b', 'model.layers.7.self_attn.k_proj.cola_a', 'model.layers.7.self_attn.k_proj.cola_b', 'model.layers.7.self_attn.v_proj.cola_a', 'model.layers.7.self_attn.v_proj.cola_b', 'model.layers.7.self_attn.o_proj.cola_a', 'model.layers.7.self_attn.o_proj.cola_b', 'model.layers.7.mlp.gate_proj.cola_a', 'model.layers.7.mlp.gate_proj.cola_b', 'model.layers.7.mlp.up_proj.cola_a', 'model.layers.7.mlp.up_proj.cola_b', 'model.layers.7.mlp.down_proj.cola_a', 'model.layers.7.mlp.down_proj.cola_b', 'model.layers.7.input_layernorm.weight', 'model.layers.7.post_attention_layernorm.weight', 'model.norm.weight', 'lm_head.weight']

2025-05-02 23:05:17.118 | INFO     | __main__:main:504 - Total params: 42.77M
2025-05-02 23:05:17.118 | INFO     | __main__:main:507 - Total non-low-rank parameters: 32.78M
2025-05-02 23:05:17.118 | INFO     | __main__:main:512 - Total low-rank parameters: 9.99M
2025-05-02 23:05:17.119 | INFO     | __main__:main:516 - Trainable params: 42.77M
2025-05-02 23:05:17.119 | INFO     | __main__:main:519 - Saving model to checkpoints/cola_60m-2025-05-02-23-05-06 every 10000 update steps
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
Update steps:   0%|                                   | 0/10000 [00:00<?, ?it/s]2025-05-02 23:05:17.368 | INFO     | __main__:main:573 - Maximum memory allocated before training: 173102592 bytes

2025-05-02 23:05:40.733 | INFO     | __main__:main:759 - Per-example grad norm: [5.8125, 3.984375, 5.0, 3.46875, 3.921875, 3.96875, 4.96875, 5.9375, 6.03125, 5.4375, 4.46875, 4.59375, 3.5625, 4.40625, 3.703125, 4.15625, 3.703125, 3.796875, 4.46875, 4.46875, 5.3125, 3.8125, 3.9375, 6.15625, 4.5, 4.4375, 5.53125, 3.84375, 3.578125, 4.0, 3.796875, 4.1875, 4.4375, 4.21875, 4.03125, 4.53125, 3.765625, 4.03125, 4.0, 4.1875, 4.96875, 3.890625, 4.90625, 5.65625, 5.28125, 5.03125, 3.890625, 4.21875, 4.21875, 4.125, 3.59375, 3.859375, 3.875, 4.71875, 4.1875, 4.3125, 4.09375, 4.1875, 3.84375, 5.4375, 4.21875, 4.28125, 3.84375, 7.4375, 3.765625, 4.25, 3.609375, 3.796875, 4.90625, 5.21875, 4.375, 3.765625, 3.765625, 4.125, 3.75, 3.5, 3.28125, 4.28125, 4.21875, 4.96875, 4.0, 3.859375, 3.890625, 4.09375, 3.625, 4.34375, 5.0625, 4.3125, 4.25, 4.6875, 4.15625, 4.09375, 3.90625, 4.09375, 4.875, 4.40625, 3.921875, 3.765625, 4.71875, 4.34375, 4.4375, 4.75, 4.5625, 4.46875, 4.40625, 5.8125, 4.28125, 3.53125, 3.84375, 4.28125, 3.890625, 3.75, 3.6875, 4.25, 4.25, 3.5625, 5.375, 4.8125, 3.890625, 4.59375, 3.703125, 4.59375, 4.28125, 3.796875, 4.46875, 5.65625, 3.484375, 3.71875, 4.21875, 3.546875, 3.890625, 4.65625, 4.03125, 3.875, 4.15625, 3.796875, 4.09375, 3.890625, 3.890625, 4.3125, 4.0625, 3.75, 4.6875, 4.6875, 4.09375, 4.8125, 3.828125, 4.8125, 3.890625, 4.375, 3.765625, 3.515625, 4.75, 4.15625, 3.71875, 4.15625, 5.1875, 4.84375, 5.3125, 5.0625, 3.65625, 3.78125, 3.984375, 4.53125, 4.625, 4.09375, 3.453125, 5.375, 5.875, 3.71875, 4.875, 4.3125, 4.34375, 4.71875, 5.59375, 3.921875, 3.96875, 3.484375, 4.34375, 3.890625, 3.75, 4.5625, 3.4375, 5.0625, 3.765625, 4.09375, 3.96875, 4.09375, 4.21875, 4.5625, 4.96875, 4.4375, 5.46875, 3.609375, 3.6875, 3.875, 4.125, 5.4375, 5.1875, 3.4375, 14.125, 4.40625, 4.34375, 3.984375, 3.53125, 4.21875, 6.78125, 4.0625, 3.96875, 3.609375, 4.53125, 4.75, 5.15625, 4.75, 4.15625, 4.9375, 4.21875, 3.921875, 4.6875, 5.125, 4.34375, 4.0625, 5.0, 3.65625, 3.625, 3.8125, 5.78125, 3.984375, 3.984375, 4.125, 3.859375, 4.46875, 4.46875, 4.0625, 3.515625, 3.875, 3.53125, 3.71875, 3.6875, 4.78125, 3.859375, 4.96875, 3.75, 4.5625, 4.5, 4.46875, 3.609375, 5.34375, 4.125, 4.5, 3.546875, 4.59375, 4.15625, 4.09375, 4.09375, 3.78125, 3.640625, 5.25, 4.21875, 3.796875, 4.78125, 3.703125, 4.4375, 4.125, 3.625, 3.5, 3.8125, 4.21875, 4.21875, 4.28125, 6.0625, 4.53125, 3.6875, 3.765625, 3.78125, 3.65625, 4.5625, 4.03125, 3.875, 3.9375, 4.25, 4.125, 4.03125, 4.15625, 3.9375, 4.59375, 4.65625, 4.28125, 4.3125, 4.21875, 3.703125, 6.09375, 4.28125, 4.1875, 4.6875, 3.59375, 3.859375, 4.625, 3.6875, 3.40625, 5.15625, 4.28125, 4.0625, 4.78125, 3.59375, 4.5, 3.875, 4.09375, 5.625, 7.15625, 3.71875, 4.125, 4.375, 4.65625, 3.578125, 3.859375, 3.96875, 3.765625, 3.65625, 3.953125]
2025-05-02 23:05:40.734 | INFO     | __main__:main:760 - Gradient noise scale (B): 1.625
2025-05-02 23:05:40.734 | INFO     | __main__:main:761 - Trace of covariance: 11.8125
2025-05-02 23:05:40.734 | INFO     | __main__:main:762 - Norm squared of mean gradient: 7.28125
Update steps:   0%|                                   | 0/10000 [00:23<?, ?it/s]
[1;34mwandb[0m: 
[1;34mwandb[0m: üöÄ View run [33mcola_60m-LR-0.006-ZO[0m at: [34mhttps://wandb.ai/yequan_zhao-university-of-california-santa-barbara/cola/runs/t8t4a4as[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250502_230506-t8t4a4as/logs[0m
