W0501 16:46:20.417000 184101 torch/distributed/run.py:792] 
W0501 16:46:20.417000 184101 torch/distributed/run.py:792] *****************************************
W0501 16:46:20.417000 184101 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0501 16:46:20.417000 184101 torch/distributed/run.py:792] *****************************************
Starting script
2025-05-01 16:46:27.043 | INFO     | __main__:main:238 - Global rank 0, local rank 0, device: 0
2025-05-01 16:46:27.050 | INFO     | __main__:main:249 - Process group initialized
2025-05-01 16:46:27.050 | INFO     | __main__:main:261 - 1-2-64-32
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
Starting script
2025-05-01 16:46:27.158 | INFO     | __main__:main:238 - Global rank 1, local rank 1, device: 1
2025-05-01 16:46:27.165 | INFO     | __main__:main:249 - Process group initialized
wandb: Currently logged in as: yequan_zhao (yequan_zhao-university-of-california-santa-barbara) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.9
wandb: Run data is saved locally in /workspace/wandb/run-20250501_164627-g33wthh9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cola_60m-LR-0.006-ZO
wandb: ‚≠êÔ∏è View project at https://wandb.ai/yequan_zhao-university-of-california-santa-barbara/cola
wandb: üöÄ View run at https://wandb.ai/yequan_zhao-university-of-california-santa-barbara/cola/runs/g33wthh9
2025-05-01 16:46:28.070 | INFO     | __main__:main:287 - Using dist with rank 0 (only rank 0 will log)
2025-05-01 16:46:28.071 | INFO     | __main__:main:288 - ****************************************
2025-05-01 16:46:28.071 | INFO     | __main__:main:289 - Starting training with the arguments
2025-05-01 16:46:28.071 | INFO     | __main__:main:291 - model_type                     cola
2025-05-01 16:46:28.071 | INFO     | __main__:main:291 - run_name                       cola_60m-LR-0.006-ZO
2025-05-01 16:46:28.071 | INFO     | __main__:main:291 - wandb_project                  cola
2025-05-01 16:46:28.071 | INFO     | __main__:main:291 - model_config                   cola_configs/cola_60m.json
2025-05-01 16:46:28.071 | INFO     | __main__:main:291 - offline_mode                   False
2025-05-01 16:46:28.071 | INFO     | __main__:main:291 - continue_from                  None
2025-05-01 16:46:28.071 | INFO     | __main__:main:291 - batch_size                     32
2025-05-01 16:46:28.071 | INFO     | __main__:main:291 - gradient_accumulation          1
2025-05-01 16:46:28.071 | INFO     | __main__:main:291 - total_batch_size               64
2025-05-01 16:46:28.071 | INFO     | __main__:main:291 - max_length                     256
2025-05-01 16:46:28.071 | INFO     | __main__:main:291 - optimizer                      adamw
2025-05-01 16:46:28.071 | INFO     | __main__:main:291 - lr                             0.006
2025-05-01 16:46:28.071 | INFO     | __main__:main:291 - scheduler                      cosine
2025-05-01 16:46:28.071 | INFO     | __main__:main:291 - min_lr_ratio                   0.1
2025-05-01 16:46:28.071 | INFO     | __main__:main:291 - activation_checkpointing       False
2025-05-01 16:46:28.071 | INFO     | __main__:main:291 - weight_decay                   0.01
2025-05-01 16:46:28.072 | INFO     | __main__:main:291 - warmup_steps                   2000
2025-05-01 16:46:28.072 | INFO     | __main__:main:291 - eval_every                     1000
2025-05-01 16:46:28.072 | INFO     | __main__:main:291 - num_training_steps             10000
2025-05-01 16:46:28.072 | INFO     | __main__:main:291 - max_train_tokens               None
2025-05-01 16:46:28.072 | INFO     | __main__:main:291 - save_every                     10000
2025-05-01 16:46:28.072 | INFO     | __main__:main:291 - save_dir                       checkpoints/cola_60m-2025-05-01-16-46-27
2025-05-01 16:46:28.072 | INFO     | __main__:main:291 - tags                           None
2025-05-01 16:46:28.072 | INFO     | __main__:main:291 - dtype                          bfloat16
2025-05-01 16:46:28.072 | INFO     | __main__:main:291 - workers                        8
2025-05-01 16:46:28.072 | INFO     | __main__:main:291 - seed                           42
2025-05-01 16:46:28.072 | INFO     | __main__:main:291 - name                           test
2025-05-01 16:46:28.072 | INFO     | __main__:main:291 - grad_clipping                  0.5
2025-05-01 16:46:28.072 | INFO     | __main__:main:291 - beta1                          0.0
2025-05-01 16:46:28.072 | INFO     | __main__:main:291 - single_gpu                     False
2025-05-01 16:46:28.072 | INFO     | __main__:main:291 - ZO_Estim                       True
2025-05-01 16:46:28.072 | INFO     | __main__:main:292 - ****************************************
layer model.layers.0.self_attn.q_proj
layer model.layers.0.self_attn.k_proj
layer model.layers.0.self_attn.v_proj
layer model.layers.0.self_attn.o_proj
layer model.layers.0.mlp.gate_proj
layer model.layers.0.mlp.up_proj
layer model.layers.0.mlp.down_proj
layer model.layers.1.self_attn.q_proj
layer model.layers.1.self_attn.k_proj
layer model.layers.1.self_attn.v_proj
layer model.layers.1.self_attn.o_proj
layer model.layers.1.mlp.gate_proj
layer model.layers.1.mlp.up_proj
layer model.layers.1.mlp.down_proj
layer model.layers.2.self_attn.q_proj
layer model.layers.2.self_attn.k_proj
layer model.layers.2.self_attn.v_proj
layer model.layers.2.self_attn.o_proj
layer model.layers.2.mlp.gate_proj
layer model.layers.2.mlp.up_proj
layer model.layers.2.mlp.down_proj
layer model.layers.3.self_attn.q_proj
layer model.layers.3.self_attn.k_proj
layer model.layers.3.self_attn.v_proj
layer model.layers.3.self_attn.o_proj
layer model.layers.3.mlp.gate_proj
layer model.layers.3.mlp.up_proj
layer model.layers.3.mlp.down_proj
layer model.layers.4.self_attn.q_proj
layer model.layers.4.self_attn.k_proj
layer model.layers.4.self_attn.v_proj
layer model.layers.4.self_attn.o_proj
layer model.layers.4.mlp.gate_proj
layer model.layers.4.mlp.up_proj
layer model.layers.4.mlp.down_proj
layer model.layers.5.self_attn.q_proj
layer model.layers.5.self_attn.k_proj
layer model.layers.5.self_attn.v_proj
layer model.layers.5.self_attn.o_proj
layer model.layers.5.mlp.gate_proj
layer model.layers.5.mlp.up_proj
layer model.layers.5.mlp.down_proj
layer model.layers.6.self_attn.q_proj
layer model.layers.6.self_attn.k_proj
layer model.layers.6.self_attn.v_proj
layer model.layers.6.self_attn.o_proj
layer model.layers.6.mlp.gate_proj
layer model.layers.6.mlp.up_proj
layer model.layers.6.mlp.down_proj
layer model.layers.7.self_attn.q_proj
layer model.layers.7.self_attn.k_proj
layer model.layers.7.self_attn.v_proj
layer model.layers.7.self_attn.o_proj
layer model.layers.7.mlp.gate_proj
layer model.layers.7.mlp.up_proj
layer model.layers.7.mlp.down_proj
2025-05-01 16:46:36.693 | INFO     | __main__:main:302 - Shuffling data with seed 42
2025-05-01 16:46:37.400 | WARNING  | __main__:main:390 - Did not find training state in None, global step will start from zero
2025-05-01 16:46:37.401 | INFO     | __main__:main:393 - ****************************************
layer model.layers.0.self_attn.q_proj
layer model.layers.0.self_attn.k_proj
layer model.layers.0.self_attn.v_proj
layer model.layers.0.self_attn.o_proj
layer model.layers.0.mlp.gate_proj
layer model.layers.0.mlp.up_proj
layer model.layers.0.mlp.down_proj
layer model.layers.1.self_attn.q_proj
layer model.layers.1.self_attn.k_proj
layer model.layers.1.self_attn.v_proj
layer model.layers.1.self_attn.o_proj
layer model.layers.1.mlp.gate_proj
layer model.layers.1.mlp.up_proj
layer model.layers.1.mlp.down_proj
layer model.layers.2.self_attn.q_proj
layer model.layers.2.self_attn.k_proj
layer model.layers.2.self_attn.v_proj
layer model.layers.2.self_attn.o_proj
layer model.layers.2.mlp.gate_proj
layer model.layers.2.mlp.up_proj
layer model.layers.2.mlp.down_proj
layer model.layers.3.self_attn.q_proj
layer model.layers.3.self_attn.k_proj
layer model.layers.3.self_attn.v_proj
layer model.layers.3.self_attn.o_proj
layer model.layers.3.mlp.gate_proj
layer model.layers.3.mlp.up_proj
layer model.layers.3.mlp.down_proj
layer model.layers.4.self_attn.q_proj
layer model.layers.4.self_attn.k_proj
layer model.layers.4.self_attn.v_proj
layer model.layers.4.self_attn.o_proj
layer model.layers.4.mlp.gate_proj
layer model.layers.4.mlp.up_proj
layer model.layers.4.mlp.down_proj
layer model.layers.5.self_attn.q_proj
layer model.layers.5.self_attn.k_proj
layer model.layers.5.self_attn.v_proj
layer model.layers.5.self_attn.o_proj
layer model.layers.5.mlp.gate_proj
layer model.layers.5.mlp.up_proj
layer model.layers.5.mlp.down_proj
layer model.layers.6.self_attn.q_proj
layer model.layers.6.self_attn.k_proj
layer model.layers.6.self_attn.v_proj
layer model.layers.6.self_attn.o_proj
layer model.layers.6.mlp.gate_proj
layer model.layers.6.mlp.up_proj
layer model.layers.6.mlp.down_proj
layer model.layers.7.self_attn.q_proj
layer model.layers.7.self_attn.k_proj
layer model.layers.7.self_attn.v_proj
layer model.layers.7.self_attn.o_proj
layer model.layers.7.mlp.gate_proj
layer model.layers.7.mlp.up_proj
layer model.layers.7.mlp.down_proj
2025-05-01 16:46:37.993 | INFO     | __main__:main:499 - Running with cola

2025-05-01 16:46:37.994 | INFO     | __main__:main:500 - 
ColaForCausalLM(
  (model): ColaModel(
    (embed_tokens): Embedding(32000, 512, padding_idx=0)
    (layers): ModuleList(
      (0-7): 8 x ColaDecoderLayer(
        (self_attn): ColaSdpaAttention(
          (q_proj): ColaLayer(
            cola_a: torch.Size([512, 128]), cola_b: torch.Size([128, 512]), bias: False
            (lr_act): SiLU()
          )
          (k_proj): ColaLayer(
            cola_a: torch.Size([512, 128]), cola_b: torch.Size([128, 512]), bias: False
            (lr_act): SiLU()
          )
          (v_proj): ColaLayer(
            cola_a: torch.Size([512, 128]), cola_b: torch.Size([128, 512]), bias: False
            (lr_act): SiLU()
          )
          (o_proj): ColaLayer(
            cola_a: torch.Size([512, 128]), cola_b: torch.Size([128, 512]), bias: False
            (lr_act): SiLU()
          )
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): ColaMLP(
          (gate_proj): ColaLayer(
            cola_a: torch.Size([512, 128]), cola_b: torch.Size([128, 1376]), bias: False
            (lr_act): SiLU()
          )
          (up_proj): ColaLayer(
            cola_a: torch.Size([512, 128]), cola_b: torch.Size([128, 1376]), bias: False
            (lr_act): SiLU()
          )
          (down_proj): ColaLayer(
            cola_a: torch.Size([1376, 128]), cola_b: torch.Size([128, 512]), bias: False
            (lr_act): SiLU()
          )
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((512,), eps=1e-06)
        (post_attention_layernorm): LlamaRMSNorm((512,), eps=1e-06)
      )
    )
    (norm): LlamaRMSNorm((512,), eps=1e-06)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=512, out_features=32000, bias=False)
)

2025-05-01 16:46:37.994 | INFO     | __main__:main:501 - All params: 
['model.embed_tokens.weight', 'model.layers.0.self_attn.q_proj.cola_a', 'model.layers.0.self_attn.q_proj.cola_b', 'model.layers.0.self_attn.k_proj.cola_a', 'model.layers.0.self_attn.k_proj.cola_b', 'model.layers.0.self_attn.v_proj.cola_a', 'model.layers.0.self_attn.v_proj.cola_b', 'model.layers.0.self_attn.o_proj.cola_a', 'model.layers.0.self_attn.o_proj.cola_b', 'model.layers.0.mlp.gate_proj.cola_a', 'model.layers.0.mlp.gate_proj.cola_b', 'model.layers.0.mlp.up_proj.cola_a', 'model.layers.0.mlp.up_proj.cola_b', 'model.layers.0.mlp.down_proj.cola_a', 'model.layers.0.mlp.down_proj.cola_b', 'model.layers.0.input_layernorm.weight', 'model.layers.0.post_attention_layernorm.weight', 'model.layers.1.self_attn.q_proj.cola_a', 'model.layers.1.self_attn.q_proj.cola_b', 'model.layers.1.self_attn.k_proj.cola_a', 'model.layers.1.self_attn.k_proj.cola_b', 'model.layers.1.self_attn.v_proj.cola_a', 'model.layers.1.self_attn.v_proj.cola_b', 'model.layers.1.self_attn.o_proj.cola_a', 'model.layers.1.self_attn.o_proj.cola_b', 'model.layers.1.mlp.gate_proj.cola_a', 'model.layers.1.mlp.gate_proj.cola_b', 'model.layers.1.mlp.up_proj.cola_a', 'model.layers.1.mlp.up_proj.cola_b', 'model.layers.1.mlp.down_proj.cola_a', 'model.layers.1.mlp.down_proj.cola_b', 'model.layers.1.input_layernorm.weight', 'model.layers.1.post_attention_layernorm.weight', 'model.layers.2.self_attn.q_proj.cola_a', 'model.layers.2.self_attn.q_proj.cola_b', 'model.layers.2.self_attn.k_proj.cola_a', 'model.layers.2.self_attn.k_proj.cola_b', 'model.layers.2.self_attn.v_proj.cola_a', 'model.layers.2.self_attn.v_proj.cola_b', 'model.layers.2.self_attn.o_proj.cola_a', 'model.layers.2.self_attn.o_proj.cola_b', 'model.layers.2.mlp.gate_proj.cola_a', 'model.layers.2.mlp.gate_proj.cola_b', 'model.layers.2.mlp.up_proj.cola_a', 'model.layers.2.mlp.up_proj.cola_b', 'model.layers.2.mlp.down_proj.cola_a', 'model.layers.2.mlp.down_proj.cola_b', 'model.layers.2.input_layernorm.weight', 'model.layers.2.post_attention_layernorm.weight', 'model.layers.3.self_attn.q_proj.cola_a', 'model.layers.3.self_attn.q_proj.cola_b', 'model.layers.3.self_attn.k_proj.cola_a', 'model.layers.3.self_attn.k_proj.cola_b', 'model.layers.3.self_attn.v_proj.cola_a', 'model.layers.3.self_attn.v_proj.cola_b', 'model.layers.3.self_attn.o_proj.cola_a', 'model.layers.3.self_attn.o_proj.cola_b', 'model.layers.3.mlp.gate_proj.cola_a', 'model.layers.3.mlp.gate_proj.cola_b', 'model.layers.3.mlp.up_proj.cola_a', 'model.layers.3.mlp.up_proj.cola_b', 'model.layers.3.mlp.down_proj.cola_a', 'model.layers.3.mlp.down_proj.cola_b', 'model.layers.3.input_layernorm.weight', 'model.layers.3.post_attention_layernorm.weight', 'model.layers.4.self_attn.q_proj.cola_a', 'model.layers.4.self_attn.q_proj.cola_b', 'model.layers.4.self_attn.k_proj.cola_a', 'model.layers.4.self_attn.k_proj.cola_b', 'model.layers.4.self_attn.v_proj.cola_a', 'model.layers.4.self_attn.v_proj.cola_b', 'model.layers.4.self_attn.o_proj.cola_a', 'model.layers.4.self_attn.o_proj.cola_b', 'model.layers.4.mlp.gate_proj.cola_a', 'model.layers.4.mlp.gate_proj.cola_b', 'model.layers.4.mlp.up_proj.cola_a', 'model.layers.4.mlp.up_proj.cola_b', 'model.layers.4.mlp.down_proj.cola_a', 'model.layers.4.mlp.down_proj.cola_b', 'model.layers.4.input_layernorm.weight', 'model.layers.4.post_attention_layernorm.weight', 'model.layers.5.self_attn.q_proj.cola_a', 'model.layers.5.self_attn.q_proj.cola_b', 'model.layers.5.self_attn.k_proj.cola_a', 'model.layers.5.self_attn.k_proj.cola_b', 'model.layers.5.self_attn.v_proj.cola_a', 'model.layers.5.self_attn.v_proj.cola_b', 'model.layers.5.self_attn.o_proj.cola_a', 'model.layers.5.self_attn.o_proj.cola_b', 'model.layers.5.mlp.gate_proj.cola_a', 'model.layers.5.mlp.gate_proj.cola_b', 'model.layers.5.mlp.up_proj.cola_a', 'model.layers.5.mlp.up_proj.cola_b', 'model.layers.5.mlp.down_proj.cola_a', 'model.layers.5.mlp.down_proj.cola_b', 'model.layers.5.input_layernorm.weight', 'model.layers.5.post_attention_layernorm.weight', 'model.layers.6.self_attn.q_proj.cola_a', 'model.layers.6.self_attn.q_proj.cola_b', 'model.layers.6.self_attn.k_proj.cola_a', 'model.layers.6.self_attn.k_proj.cola_b', 'model.layers.6.self_attn.v_proj.cola_a', 'model.layers.6.self_attn.v_proj.cola_b', 'model.layers.6.self_attn.o_proj.cola_a', 'model.layers.6.self_attn.o_proj.cola_b', 'model.layers.6.mlp.gate_proj.cola_a', 'model.layers.6.mlp.gate_proj.cola_b', 'model.layers.6.mlp.up_proj.cola_a', 'model.layers.6.mlp.up_proj.cola_b', 'model.layers.6.mlp.down_proj.cola_a', 'model.layers.6.mlp.down_proj.cola_b', 'model.layers.6.input_layernorm.weight', 'model.layers.6.post_attention_layernorm.weight', 'model.layers.7.self_attn.q_proj.cola_a', 'model.layers.7.self_attn.q_proj.cola_b', 'model.layers.7.self_attn.k_proj.cola_a', 'model.layers.7.self_attn.k_proj.cola_b', 'model.layers.7.self_attn.v_proj.cola_a', 'model.layers.7.self_attn.v_proj.cola_b', 'model.layers.7.self_attn.o_proj.cola_a', 'model.layers.7.self_attn.o_proj.cola_b', 'model.layers.7.mlp.gate_proj.cola_a', 'model.layers.7.mlp.gate_proj.cola_b', 'model.layers.7.mlp.up_proj.cola_a', 'model.layers.7.mlp.up_proj.cola_b', 'model.layers.7.mlp.down_proj.cola_a', 'model.layers.7.mlp.down_proj.cola_b', 'model.layers.7.input_layernorm.weight', 'model.layers.7.post_attention_layernorm.weight', 'model.norm.weight', 'lm_head.weight']

2025-05-01 16:46:37.994 | INFO     | __main__:main:504 - Total params: 42.77M
2025-05-01 16:46:37.995 | INFO     | __main__:main:507 - Total non-low-rank parameters: 32.78M
2025-05-01 16:46:37.995 | INFO     | __main__:main:512 - Total low-rank parameters: 9.99M
2025-05-01 16:46:37.995 | INFO     | __main__:main:516 - Trainable params: 42.77M
2025-05-01 16:46:37.995 | INFO     | __main__:main:519 - Saving model to checkpoints/cola_60m-2025-05-01-16-46-27 every 10000 update steps
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
Update steps:   0%|                                   | 0/10000 [00:00<?, ?it/s]2025-05-01 16:46:38.252 | INFO     | __main__:main:573 - Maximum memory allocated before training: 173102592 bytes

model.layers.0.self_attn.q_proj out_dimension=131072.0
model.layers.0.self_attn.k_proj out_dimension=131072.0
model.layers.0.self_attn.v_proj out_dimension=131072.0
model.layers.0.self_attn.o_proj out_dimension=131072.0
model.layers.0.mlp.gate_proj out_dimension=352256.0
model.layers.0.mlp.up_proj out_dimension=352256.0
model.layers.0.mlp.down_proj out_dimension=131072.0
model.layers.1.self_attn.q_proj out_dimension=131072.0
model.layers.1.self_attn.k_proj out_dimension=131072.0
model.layers.1.self_attn.v_proj out_dimension=131072.0
model.layers.1.self_attn.o_proj out_dimension=131072.0
model.layers.1.mlp.gate_proj out_dimension=352256.0
model.layers.1.mlp.up_proj out_dimension=352256.0
model.layers.1.mlp.down_proj out_dimension=131072.0
model.layers.2.self_attn.q_proj out_dimension=131072.0
model.layers.2.self_attn.k_proj out_dimension=131072.0
model.layers.2.self_attn.v_proj out_dimension=131072.0
model.layers.2.self_attn.o_proj out_dimension=131072.0
model.layers.2.mlp.gate_proj out_dimension=352256.0
model.layers.2.mlp.up_proj out_dimension=352256.0
model.layers.2.mlp.down_proj out_dimension=131072.0
model.layers.3.self_attn.q_proj out_dimension=131072.0
model.layers.3.self_attn.k_proj out_dimension=131072.0
model.layers.3.self_attn.v_proj out_dimension=131072.0
model.layers.3.self_attn.o_proj out_dimension=131072.0
model.layers.3.mlp.gate_proj out_dimension=352256.0
model.layers.3.mlp.up_proj out_dimension=352256.0
model.layers.3.mlp.down_proj out_dimension=131072.0
model.layers.4.self_attn.q_proj out_dimension=131072.0
model.layers.4.self_attn.k_proj out_dimension=131072.0
model.layers.4.self_attn.v_proj out_dimension=131072.0
model.layers.4.self_attn.o_proj out_dimension=131072.0
model.layers.4.mlp.gate_proj out_dimension=352256.0
model.layers.4.mlp.up_proj out_dimension=352256.0
model.layers.4.mlp.down_proj out_dimension=131072.0
model.layers.5.self_attn.q_proj out_dimension=131072.0
model.layers.5.self_attn.k_proj out_dimension=131072.0
model.layers.5.self_attn.v_proj out_dimension=131072.0
model.layers.5.self_attn.o_proj out_dimension=131072.0
model.layers.5.mlp.gate_proj out_dimension=352256.0
model.layers.5.mlp.up_proj out_dimension=352256.0
model.layers.5.mlp.down_proj out_dimension=131072.0
model.layers.6.self_attn.q_proj out_dimension=131072.0
model.layers.6.self_attn.k_proj out_dimension=131072.0
model.layers.6.self_attn.v_proj out_dimension=131072.0
model.layers.6.self_attn.o_proj out_dimension=131072.0
model.layers.6.mlp.gate_proj out_dimension=352256.0
model.layers.6.mlp.up_proj out_dimension=352256.0
model.layers.6.mlp.down_proj out_dimension=131072.0
model.layers.7.self_attn.q_proj out_dimension=131072.0
model.layers.7.self_attn.k_proj out_dimension=131072.0
model.layers.7.self_attn.v_proj out_dimension=131072.0
model.layers.7.self_attn.o_proj out_dimension=131072.0
model.layers.7.mlp.gate_proj out_dimension=352256.0
model.layers.7.mlp.up_proj out_dimension=352256.0
model.layers.7.mlp.down_proj out_dimension=131072.0
ZO_dimension= 10878976
model.layers.0.self_attn.q_proj out_dimension=131072.0
model.layers.0.self_attn.k_proj out_dimension=131072.0
model.layers.0.self_attn.v_proj out_dimension=131072.0
model.layers.0.self_attn.o_proj out_dimension=131072.0
model.layers.0.mlp.gate_proj out_dimension=352256.0
model.layers.0.mlp.up_proj out_dimension=352256.0
model.layers.0.mlp.down_proj out_dimension=131072.0
model.layers.1.self_attn.q_proj out_dimension=131072.0
model.layers.1.self_attn.k_proj out_dimension=131072.0
model.layers.1.self_attn.v_proj out_dimension=131072.0
model.layers.1.self_attn.o_proj out_dimension=131072.0
model.layers.1.mlp.gate_proj out_dimension=352256.0
model.layers.1.mlp.up_proj out_dimension=352256.0
model.layers.1.mlp.down_proj out_dimension=131072.0
model.layers.2.self_attn.q_proj out_dimension=131072.0
model.layers.2.self_attn.k_proj out_dimension=131072.0
model.layers.2.self_attn.v_proj out_dimension=131072.0
model.layers.2.self_attn.o_proj out_dimension=131072.0
model.layers.2.mlp.gate_proj out_dimension=352256.0
model.layers.2.mlp.up_proj out_dimension=352256.0
model.layers.2.mlp.down_proj out_dimension=131072.0
model.layers.3.self_attn.q_proj out_dimension=131072.0
model.layers.3.self_attn.k_proj out_dimension=131072.0
model.layers.3.self_attn.v_proj out_dimension=131072.0
model.layers.3.self_attn.o_proj out_dimension=131072.0
model.layers.3.mlp.gate_proj out_dimension=352256.0
model.layers.3.mlp.up_proj out_dimension=352256.0
model.layers.3.mlp.down_proj out_dimension=131072.0
model.layers.4.self_attn.q_proj out_dimension=131072.0
model.layers.4.self_attn.k_proj out_dimension=131072.0
model.layers.4.self_attn.v_proj out_dimension=131072.0
model.layers.4.self_attn.o_proj out_dimension=131072.0
model.layers.4.mlp.gate_proj out_dimension=352256.0
model.layers.4.mlp.up_proj out_dimension=352256.0
model.layers.4.mlp.down_proj out_dimension=131072.0
model.layers.5.self_attn.q_proj out_dimension=131072.0
model.layers.5.self_attn.k_proj out_dimension=131072.0
model.layers.5.self_attn.v_proj out_dimension=131072.0
model.layers.5.self_attn.o_proj out_dimension=131072.0
model.layers.5.mlp.gate_proj out_dimension=352256.0
model.layers.5.mlp.up_proj out_dimension=352256.0
model.layers.5.mlp.down_proj out_dimension=131072.0
model.layers.6.self_attn.q_proj out_dimension=131072.0
model.layers.6.self_attn.k_proj out_dimension=131072.0
model.layers.6.self_attn.v_proj out_dimension=131072.0
model.layers.6.self_attn.o_proj out_dimension=131072.0
model.layers.6.mlp.gate_proj out_dimension=352256.0
model.layers.6.mlp.up_proj out_dimension=352256.0
model.layers.6.mlp.down_proj out_dimension=131072.0
model.layers.7.self_attn.q_proj out_dimension=131072.0
model.layers.7.self_attn.k_proj out_dimension=131072.0
model.layers.7.self_attn.v_proj out_dimension=131072.0
model.layers.7.self_attn.o_proj out_dimension=131072.0
model.layers.7.mlp.gate_proj out_dimension=352256.0
model.layers.7.mlp.up_proj out_dimension=352256.0
model.layers.7.mlp.down_proj out_dimension=131072.0
ZO_dimension= 10878976
Update steps:   0%|                       | 1/10000 [01:16<213:30:18, 76.87s/it]Update steps:   0%|                        | 2/10000 [01:17<88:41:55, 31.94s/it]Update steps:   0%|                        | 3/10000 [01:17<48:22:11, 17.42s/it]Update steps:   0%|                        | 4/10000 [01:17<29:26:13, 10.60s/it]Update steps:   0%|                        | 5/10000 [01:17<18:57:56,  6.83s/it]Update steps:   0%|                        | 6/10000 [01:17<12:38:46,  4.56s/it]Update steps:   0%|                         | 7/10000 [01:18<8:38:54,  3.12s/it]Update steps:   0%|                         | 8/10000 [01:18<6:01:41,  2.17s/it]Update steps:   0%|                         | 9/10000 [01:18<4:16:23,  1.54s/it]Update steps:   0%|                        | 10/10000 [01:18<3:04:07,  1.11s/it]Update steps:   0%|                        | 11/10000 [01:18<2:16:17,  1.22it/s]Update steps:   0%|                        | 12/10000 [01:18<1:42:30,  1.62it/s]Update steps:   0%|                        | 13/10000 [01:18<1:18:46,  2.11it/s]Update steps:   0%|                        | 14/10000 [01:19<1:01:34,  2.70it/s]Update steps:   0%|                          | 15/10000 [01:19<50:01,  3.33it/s]Update steps:   0%|                          | 16/10000 [01:19<41:40,  3.99it/s]Update steps:   0%|                          | 17/10000 [01:19<36:55,  4.51it/s]Update steps:   0%|                          | 18/10000 [01:19<34:20,  4.85it/s]Update steps:   0%|                          | 19/10000 [01:19<31:53,  5.22it/s]Update steps:   0%|                          | 20/10000 [01:20<30:06,  5.52it/s]Update steps:   0%|                          | 21/10000 [01:20<28:23,  5.86it/s]Update steps:   0%|                          | 22/10000 [01:20<26:40,  6.24it/s]Update steps:   0%|                          | 23/10000 [01:20<26:15,  6.33it/s]Update steps:   0%|                          | 24/10000 [01:20<25:48,  6.44it/s]Update steps:   0%|                          | 25/10000 [01:20<24:48,  6.70it/s]Update steps:   0%|                          | 26/10000 [01:20<25:16,  6.58it/s]Update steps:   0%|                          | 27/10000 [01:21<25:03,  6.63it/s]Update steps:   0%|                          | 28/10000 [01:21<24:21,  6.82it/s]Update steps:   0%|                          | 29/10000 [01:21<24:33,  6.77it/s]Update steps:   0%|                          | 30/10000 [01:21<23:36,  7.04it/s]Update steps:   0%|                          | 31/10000 [01:21<23:26,  7.09it/s]Update steps:   0%|                          | 32/10000 [01:21<23:51,  6.97it/s]Update steps:   0%|                          | 33/10000 [01:21<23:49,  6.97it/s]Update steps:   0%|                          | 34/10000 [01:22<23:08,  7.18it/s]Update steps:   0%|                          | 35/10000 [01:22<22:58,  7.23it/s]Update steps:   0%|                          | 36/10000 [01:22<23:40,  7.01it/s]Update steps:   0%|                          | 37/10000 [01:22<24:12,  6.86it/s]Update steps:   0%|                          | 38/10000 [01:22<24:46,  6.70it/s]Update steps:   0%|                          | 39/10000 [01:22<24:55,  6.66it/s]Update steps:   0%|                          | 40/10000 [01:22<24:47,  6.70it/s]Update steps:   0%|                          | 41/10000 [01:23<23:51,  6.96it/s]Update steps:   0%|                          | 42/10000 [01:23<23:29,  7.07it/s]Update steps:   0%|                          | 43/10000 [01:23<24:18,  6.82it/s]Update steps:   0%|                          | 44/10000 [01:23<24:38,  6.73it/s]Update steps:   0%|                          | 45/10000 [01:23<24:28,  6.78it/s]Update steps:   0%|                          | 46/10000 [01:23<23:31,  7.05it/s]Update steps:   0%|                          | 47/10000 [01:23<23:52,  6.95it/s]Update steps:   0%|                          | 48/10000 [01:24<24:20,  6.82it/s]Update steps:   0%|‚ñè                         | 49/10000 [01:24<24:17,  6.83it/s]Update steps:   0%|‚ñè                         | 50/10000 [01:24<23:26,  7.07it/s]Update steps:   1%|‚ñè                         | 51/10000 [01:24<22:50,  7.26it/s]Update steps:   1%|‚ñè                         | 52/10000 [01:24<22:44,  7.29it/s]Update steps:   1%|‚ñè                         | 53/10000 [01:24<23:04,  7.18it/s]Update steps:   1%|‚ñè                         | 54/10000 [01:24<22:33,  7.35it/s]Update steps:   1%|‚ñè                         | 55/10000 [01:25<22:40,  7.31it/s]Update steps:   1%|‚ñè                         | 56/10000 [01:25<23:31,  7.05it/s]Update steps:   1%|‚ñè                         | 57/10000 [01:25<23:44,  6.98it/s]Update steps:   1%|‚ñè                         | 58/10000 [01:25<22:59,  7.20it/s]Update steps:   1%|‚ñè                         | 59/10000 [01:25<23:11,  7.14it/s]Update steps:   1%|‚ñè                         | 60/10000 [01:25<22:58,  7.21it/s]Update steps:   1%|‚ñè                         | 61/10000 [01:25<23:56,  6.92it/s]Update steps:   1%|‚ñè                         | 62/10000 [01:26<24:20,  6.81it/s]Update steps:   1%|‚ñè                         | 63/10000 [01:26<24:54,  6.65it/s]Update steps:   1%|‚ñè                         | 64/10000 [01:26<25:02,  6.61it/s]Update steps:   1%|‚ñè                         | 65/10000 [01:26<25:02,  6.61it/s]Update steps:   1%|‚ñè                         | 66/10000 [01:26<23:53,  6.93it/s]Update steps:   1%|‚ñè                         | 67/10000 [01:26<24:07,  6.86it/s]Update steps:   1%|‚ñè                         | 68/10000 [01:26<24:01,  6.89it/s]Update steps:   1%|‚ñè                         | 69/10000 [01:27<23:13,  7.13it/s]Update steps:   1%|‚ñè                         | 70/10000 [01:27<23:00,  7.19it/s]Update steps:   1%|‚ñè                         | 71/10000 [01:27<23:21,  7.09it/s]Update steps:   1%|‚ñè                         | 72/10000 [01:27<22:54,  7.22it/s]Update steps:   1%|‚ñè                         | 73/10000 [01:27<23:51,  6.93it/s]Update steps:   1%|‚ñè                         | 74/10000 [01:27<24:18,  6.81it/s]Update steps:   1%|‚ñè                         | 75/10000 [01:27<24:32,  6.74it/s]Update steps:   1%|‚ñè                         | 76/10000 [01:28<23:43,  6.97it/s]Update steps:   1%|‚ñè                         | 77/10000 [01:28<24:24,  6.77it/s]Update steps:   1%|‚ñè                         | 78/10000 [01:28<24:29,  6.75it/s]Update steps:   1%|‚ñè                         | 79/10000 [01:28<23:29,  7.04it/s]Update steps:   1%|‚ñè                         | 80/10000 [01:28<23:42,  6.98it/s]Update steps:   1%|‚ñè                         | 81/10000 [01:28<23:00,  7.19it/s]Update steps:   1%|‚ñè                         | 82/10000 [01:28<22:47,  7.25it/s]Update steps:   1%|‚ñè                         | 83/10000 [01:29<23:14,  7.11it/s]Update steps:   1%|‚ñè                         | 84/10000 [01:29<22:59,  7.19it/s]Update steps:   1%|‚ñè                         | 85/10000 [01:29<23:39,  6.98it/s]Update steps:   1%|‚ñè                         | 86/10000 [01:29<22:57,  7.20it/s]Update steps:   1%|‚ñè                         | 87/10000 [01:29<22:27,  7.35it/s]Update steps:   1%|‚ñè                         | 88/10000 [01:29<22:29,  7.34it/s]Update steps:   1%|‚ñè                         | 89/10000 [01:29<22:57,  7.20it/s]Update steps:   1%|‚ñè                         | 90/10000 [01:30<22:51,  7.23it/s]Update steps:   1%|‚ñè                         | 91/10000 [01:30<23:53,  6.91it/s]Update steps:   1%|‚ñè                         | 92/10000 [01:30<23:57,  6.89it/s]Update steps:   1%|‚ñè                         | 93/10000 [01:30<23:10,  7.13it/s]Update steps:   1%|‚ñè                         | 94/10000 [01:30<22:46,  7.25it/s]Update steps:   1%|‚ñè                         | 95/10000 [01:30<23:21,  7.07it/s]Update steps:   1%|‚ñè                         | 96/10000 [01:30<23:39,  6.98it/s]Update steps:   1%|‚ñé                         | 97/10000 [01:31<22:57,  7.19it/s]Update steps:   1%|‚ñé                         | 98/10000 [01:31<22:37,  7.29it/s]Update steps:   1%|‚ñé                         | 99/10000 [01:31<22:55,  7.20it/s]Update steps:   1%|‚ñé                        | 100/10000 [01:31<22:48,  7.23it/s]Update steps:   1%|‚ñè                     | 101/10000 [02:44<60:40:04, 22.06s/it]Update steps:   1%|‚ñè                     | 102/10000 [02:45<42:46:00, 15.55s/it]Update steps:   1%|‚ñè                     | 103/10000 [02:45<30:04:45, 10.94s/it]Update steps:   1%|‚ñè                     | 104/10000 [02:45<21:09:41,  7.70s/it]Update steps:   1%|‚ñè                     | 105/10000 [02:45<14:56:16,  5.43s/it]Update steps:   1%|‚ñè                     | 106/10000 [02:45<10:35:09,  3.85s/it]Update steps:   1%|‚ñè                      | 107/10000 [02:45<7:33:43,  2.75s/it]Update steps:   1%|‚ñè                      | 108/10000 [02:45<5:24:00,  1.97s/it]Update steps:   1%|‚ñé                      | 109/10000 [02:46<3:54:08,  1.42s/it]Update steps:   1%|‚ñé                      | 110/10000 [02:46<2:51:37,  1.04s/it]Update steps:   1%|‚ñé                      | 111/10000 [02:46<2:07:14,  1.30it/s]Update steps:   1%|‚ñé                      | 112/10000 [02:46<1:36:29,  1.71it/s]Update steps:   1%|‚ñé                      | 113/10000 [02:46<1:15:00,  2.20it/s]Update steps:   1%|‚ñé                        | 114/10000 [02:46<59:55,  2.75it/s]Update steps:   1%|‚ñé                        | 115/10000 [02:46<48:42,  3.38it/s]Update steps:   1%|‚ñé                        | 116/10000 [02:47<41:30,  3.97it/s]Update steps:   1%|‚ñé                        | 117/10000 [02:47<36:26,  4.52it/s]Update steps:   1%|‚ñé                        | 118/10000 [02:47<32:56,  5.00it/s]Update steps:   1%|‚ñé                        | 119/10000 [02:47<30:30,  5.40it/s]Update steps:   1%|‚ñé                        | 120/10000 [02:47<28:10,  5.84it/s]Update steps:   1%|‚ñé                        | 121/10000 [02:47<27:06,  6.07it/s]Update steps:   1%|‚ñé                        | 122/10000 [02:48<26:21,  6.24it/s]Update steps:   1%|‚ñé                        | 123/10000 [02:48<25:51,  6.37it/s]Update steps:   1%|‚ñé                        | 124/10000 [02:48<25:30,  6.45it/s]Update steps:   1%|‚ñé                        | 125/10000 [02:48<25:15,  6.51it/s]Update steps:   1%|‚ñé                        | 126/10000 [02:48<24:29,  6.72it/s]Update steps:   1%|‚ñé                        | 127/10000 [02:48<24:30,  6.71it/s]Update steps:   1%|‚ñé                        | 128/10000 [02:48<25:10,  6.53it/s]Update steps:   1%|‚ñé                        | 129/10000 [02:49<25:41,  6.40it/s]Update steps:   1%|‚ñé                        | 130/10000 [02:49<24:21,  6.75it/s]Update steps:   1%|‚ñé                        | 131/10000 [02:49<24:27,  6.73it/s]Update steps:   1%|‚ñé                        | 132/10000 [02:49<23:54,  6.88it/s]Update steps:   1%|‚ñé                        | 133/10000 [02:49<24:07,  6.81it/s]Update steps:   1%|‚ñé                        | 134/10000 [02:49<24:23,  6.74it/s]Update steps:   1%|‚ñé                        | 135/10000 [02:49<24:28,  6.72it/s]Update steps:   1%|‚ñé                        | 136/10000 [02:50<24:33,  6.69it/s]Update steps:   1%|‚ñé                        | 137/10000 [02:50<24:36,  6.68it/s]Update steps:   1%|‚ñé                        | 138/10000 [02:50<24:38,  6.67it/s]Update steps:   1%|‚ñé                        | 139/10000 [02:50<31:40,  5.19it/s]Update steps:   1%|‚ñé                        | 140/10000 [02:50<29:35,  5.55it/s]Update steps:   1%|‚ñé                        | 141/10000 [02:51<28:07,  5.84it/s]Update steps:   1%|‚ñé                        | 142/10000 [02:51<27:06,  6.06it/s]Update steps:   1%|‚ñé                        | 143/10000 [02:51<26:23,  6.23it/s]Update steps:   1%|‚ñé                        | 144/10000 [02:51<25:53,  6.35it/s]Update steps:   1%|‚ñé                        | 145/10000 [02:51<25:33,  6.42it/s]Update steps:   1%|‚ñé                        | 146/10000 [02:51<25:17,  6.49it/s]Update steps:   1%|‚ñé                        | 147/10000 [02:51<25:06,  6.54it/s]Update steps:   1%|‚ñé                        | 148/10000 [02:52<25:34,  6.42it/s]Update steps:   1%|‚ñé                        | 149/10000 [02:52<25:54,  6.34it/s]Update steps:   2%|‚ñç                        | 150/10000 [02:52<25:04,  6.55it/s]Update steps:   2%|‚ñç                        | 151/10000 [02:52<25:34,  6.42it/s]Update steps:   2%|‚ñç                        | 152/10000 [02:52<24:15,  6.77it/s]Update steps:   2%|‚ñç                        | 153/10000 [02:52<24:21,  6.74it/s]Update steps:   2%|‚ñç                        | 154/10000 [02:52<24:27,  6.71it/s]Update steps:   2%|‚ñç                        | 155/10000 [02:53<24:29,  6.70it/s]Update steps:   2%|‚ñç                        | 156/10000 [02:53<24:33,  6.68it/s]Update steps:   2%|‚ñç                        | 157/10000 [02:53<24:35,  6.67it/s]Update steps:   2%|‚ñç                        | 158/10000 [02:53<24:35,  6.67it/s]Update steps:   2%|‚ñç                        | 159/10000 [02:53<24:35,  6.67it/s]Update steps:   2%|‚ñç                        | 160/10000 [02:53<24:46,  6.62it/s]Update steps:   2%|‚ñç                        | 161/10000 [02:54<24:48,  6.61it/s]Update steps:   2%|‚ñç                        | 162/10000 [02:54<24:07,  6.80it/s]Update steps:   2%|‚ñç                        | 163/10000 [02:54<25:10,  6.51it/s]Update steps:   2%|‚ñç                        | 164/10000 [02:54<24:06,  6.80it/s]Update steps:   2%|‚ñç                        | 165/10000 [02:54<24:54,  6.58it/s]Update steps:   2%|‚ñç                        | 166/10000 [02:54<24:50,  6.60it/s]Update steps:   2%|‚ñç                        | 167/10000 [02:54<23:46,  6.89it/s]Update steps:   2%|‚ñç                        | 168/10000 [02:55<22:58,  7.13it/s]Update steps:   2%|‚ñç                        | 169/10000 [02:55<23:28,  6.98it/s]Update steps:   2%|‚ñç                        | 170/10000 [02:55<23:48,  6.88it/s]Update steps:   2%|‚ñç                        | 171/10000 [02:55<24:03,  6.81it/s]Update steps:   2%|‚ñç                        | 172/10000 [02:55<24:15,  6.75it/s]Update steps:   2%|‚ñç                        | 173/10000 [02:55<24:18,  6.74it/s]Update steps:   2%|‚ñç                        | 174/10000 [02:55<24:22,  6.72it/s]Update steps:   2%|‚ñç                        | 175/10000 [02:56<24:27,  6.70it/s]Update steps:   2%|‚ñç                        | 176/10000 [02:56<24:28,  6.69it/s]Update steps:   2%|‚ñç                        | 177/10000 [02:56<24:31,  6.68it/s]Update steps:   2%|‚ñç                        | 178/10000 [02:56<24:32,  6.67it/s]Update steps:   2%|‚ñç                        | 179/10000 [02:56<24:33,  6.67it/s]Update steps:   2%|‚ñç                        | 180/10000 [02:56<24:33,  6.66it/s]Update steps:   2%|‚ñç                        | 181/10000 [02:56<24:34,  6.66it/s]Update steps:   2%|‚ñç                        | 182/10000 [02:57<24:34,  6.66it/s]Update steps:   2%|‚ñç                        | 183/10000 [02:57<24:34,  6.66it/s]Update steps:   2%|‚ñç                        | 184/10000 [02:57<24:33,  6.66it/s]Update steps:   2%|‚ñç                        | 185/10000 [02:57<24:33,  6.66it/s]Update steps:   2%|‚ñç                        | 186/10000 [02:57<24:34,  6.66it/s]Update steps:   2%|‚ñç                        | 187/10000 [02:57<24:35,  6.65it/s]Update steps:   2%|‚ñç                        | 188/10000 [02:58<24:34,  6.66it/s]Update steps:   2%|‚ñç                        | 189/10000 [02:58<24:34,  6.65it/s]Update steps:   2%|‚ñç                        | 190/10000 [02:58<24:33,  6.66it/s]Update steps:   2%|‚ñç                        | 191/10000 [02:58<24:36,  6.65it/s]Update steps:   2%|‚ñç                        | 192/10000 [02:58<24:35,  6.65it/s]Update steps:   2%|‚ñç                        | 193/10000 [02:58<24:34,  6.65it/s]Update steps:   2%|‚ñç                        | 194/10000 [02:58<24:36,  6.64it/s]Update steps:   2%|‚ñç                        | 195/10000 [02:59<24:33,  6.65it/s]Update steps:   2%|‚ñç                        | 196/10000 [02:59<24:34,  6.65it/s]Update steps:   2%|‚ñç                        | 197/10000 [02:59<24:34,  6.65it/s]Update steps:   2%|‚ñç                        | 198/10000 [02:59<24:34,  6.65it/s]Update steps:   2%|‚ñç                        | 199/10000 [02:59<24:33,  6.65it/s]Update steps:   2%|‚ñå                        | 200/10000 [02:59<23:56,  6.82it/s]Update steps:   2%|‚ñç                     | 201/10000 [04:13<60:30:39, 22.23s/it]Update steps:   2%|‚ñç                     | 202/10000 [04:13<42:28:15, 15.60s/it]Update steps:   2%|‚ñç                     | 203/10000 [04:13<29:50:01, 10.96s/it]Update steps:   2%|‚ñç                     | 204/10000 [04:14<20:59:59,  7.72s/it]Update steps:   2%|‚ñç                     | 205/10000 [04:14<14:48:22,  5.44s/it]Update steps:   2%|‚ñç                     | 206/10000 [04:14<10:28:14,  3.85s/it]Update steps:   2%|‚ñç                      | 207/10000 [04:14<7:26:13,  2.73s/it]Update steps:   2%|‚ñç                      | 208/10000 [04:14<5:18:50,  1.95s/it]Update steps:   2%|‚ñç                      | 209/10000 [04:14<3:49:33,  1.41s/it]Update steps:   2%|‚ñç                      | 210/10000 [04:14<2:47:04,  1.02s/it]Update steps:   2%|‚ñç                      | 211/10000 [04:14<2:03:17,  1.32it/s]Update steps:   2%|‚ñç                      | 212/10000 [04:15<1:32:39,  1.76it/s]Update steps:   2%|‚ñç                      | 213/10000 [04:15<1:11:12,  2.29it/s]Update steps:   2%|‚ñå                        | 214/10000 [04:15<56:18,  2.90it/s]Update steps:   2%|‚ñå                        | 215/10000 [04:15<45:51,  3.56it/s]Update steps:   2%|‚ñå                        | 216/10000 [04:15<38:31,  4.23it/s]Update steps:   2%|‚ñå                        | 217/10000 [04:15<33:26,  4.88it/s]Update steps:   2%|‚ñå                        | 218/10000 [04:15<29:46,  5.47it/s]Update steps:   2%|‚ñå                        | 219/10000 [04:15<27:15,  5.98it/s]Update steps:   2%|‚ñå                        | 220/10000 [04:16<25:26,  6.41it/s]Update steps:   2%|‚ñå                        | 221/10000 [04:16<24:12,  6.73it/s]Update steps:   2%|‚ñå                        | 222/10000 [04:16<23:20,  6.98it/s]Update steps:   2%|‚ñå                        | 223/10000 [04:16<22:43,  7.17it/s]Update steps:   2%|‚ñå                        | 224/10000 [04:16<22:16,  7.32it/s]Update steps:   2%|‚ñå                        | 225/10000 [04:16<21:57,  7.42it/s]Update steps:   2%|‚ñå                        | 226/10000 [04:16<21:44,  7.49it/s]Update steps:   2%|‚ñå                        | 227/10000 [04:17<21:36,  7.54it/s]Update steps:   2%|‚ñå                        | 228/10000 [04:17<21:33,  7.56it/s]Update steps:   2%|‚ñå                        | 229/10000 [04:17<21:30,  7.57it/s]Update steps:   2%|‚ñå                        | 230/10000 [04:17<21:25,  7.60it/s]Update steps:   2%|‚ñå                        | 231/10000 [04:17<21:22,  7.62it/s]Update steps:   2%|‚ñå                        | 232/10000 [04:17<21:21,  7.62it/s]Update steps:   2%|‚ñå                        | 233/10000 [04:17<21:18,  7.64it/s]Update steps:   2%|‚ñå                        | 234/10000 [04:17<21:17,  7.65it/s]Update steps:   2%|‚ñå                        | 235/10000 [04:18<21:17,  7.64it/s]Update steps:   2%|‚ñå                        | 236/10000 [04:18<21:19,  7.63it/s]Update steps:   2%|‚ñå                        | 237/10000 [04:18<21:23,  7.60it/s]Update steps:   2%|‚ñå                        | 238/10000 [04:18<21:52,  7.44it/s]Update steps:   2%|‚ñå                        | 239/10000 [04:18<21:44,  7.48it/s]Update steps:   2%|‚ñå                        | 240/10000 [04:18<22:47,  7.14it/s]Update steps:   2%|‚ñå                        | 241/10000 [04:18<22:23,  7.27it/s]Update steps:   2%|‚ñå                        | 242/10000 [04:19<22:03,  7.37it/s]Update steps:   2%|‚ñå                        | 243/10000 [04:19<22:01,  7.39it/s]Update steps:   2%|‚ñå                        | 244/10000 [04:19<21:47,  7.46it/s]Update steps:   2%|‚ñå                        | 245/10000 [04:19<21:39,  7.51it/s]Update steps:   2%|‚ñå                        | 246/10000 [04:19<21:33,  7.54it/s]Update steps:   2%|‚ñå                        | 247/10000 [04:19<21:28,  7.57it/s]Update steps:   2%|‚ñå                        | 248/10000 [04:19<21:25,  7.59it/s]Update steps:   2%|‚ñå                        | 249/10000 [04:19<21:23,  7.59it/s]Update steps:   2%|‚ñã                        | 250/10000 [04:20<21:28,  7.57it/s]Update steps:   3%|‚ñã                        | 251/10000 [04:20<21:24,  7.59it/s]Update steps:   3%|‚ñã                        | 252/10000 [04:20<21:21,  7.61it/s]Update steps:   3%|‚ñã                        | 253/10000 [04:20<21:24,  7.59it/s]Update steps:   3%|‚ñã                        | 254/10000 [04:20<21:19,  7.62it/s]Update steps:   3%|‚ñã                        | 255/10000 [04:20<21:21,  7.61it/s]Update steps:   3%|‚ñã                        | 256/10000 [04:20<21:26,  7.57it/s]Update steps:   3%|‚ñã                        | 257/10000 [04:21<21:22,  7.60it/s]Update steps:   3%|‚ñã                        | 258/10000 [04:21<21:20,  7.61it/s]Update steps:   3%|‚ñã                        | 259/10000 [04:21<21:20,  7.61it/s]Update steps:   3%|‚ñã                        | 260/10000 [04:21<21:24,  7.58it/s]Update steps:   3%|‚ñã                        | 261/10000 [04:21<21:20,  7.60it/s]Update steps:   3%|‚ñã                        | 262/10000 [04:21<21:21,  7.60it/s]Update steps:   3%|‚ñã                        | 263/10000 [04:21<21:26,  7.57it/s]Update steps:   3%|‚ñã                        | 264/10000 [04:21<21:21,  7.60it/s]Update steps:   3%|‚ñã                        | 265/10000 [04:22<21:20,  7.60it/s]Update steps:   3%|‚ñã                        | 266/10000 [04:22<21:18,  7.62it/s]Update steps:   3%|‚ñã                        | 267/10000 [04:22<21:20,  7.60it/s]Update steps:   3%|‚ñã                        | 268/10000 [04:22<21:20,  7.60it/s]Update steps:   3%|‚ñã                        | 269/10000 [04:22<21:20,  7.60it/s]Update steps:   3%|‚ñã                        | 270/10000 [04:22<21:26,  7.56it/s]Update steps:   3%|‚ñã                        | 271/10000 [04:22<21:23,  7.58it/s]Update steps:   3%|‚ñã                        | 272/10000 [04:22<21:23,  7.58it/s]Update steps:   3%|‚ñã                        | 273/10000 [04:23<21:22,  7.59it/s]Update steps:   3%|‚ñã                        | 274/10000 [04:23<21:22,  7.58it/s]Update steps:   3%|‚ñã                        | 275/10000 [04:23<21:22,  7.59it/s]Update steps:   3%|‚ñã                        | 276/10000 [04:23<21:22,  7.58it/s]Update steps:   3%|‚ñã                        | 277/10000 [04:23<21:21,  7.59it/s]Update steps:   3%|‚ñã                        | 278/10000 [04:23<21:20,  7.59it/s]Update steps:   3%|‚ñã                        | 279/10000 [04:23<21:20,  7.59it/s]Update steps:   3%|‚ñã                        | 280/10000 [04:24<21:20,  7.59it/s]Update steps:   3%|‚ñã                        | 281/10000 [04:24<21:18,  7.60it/s]Update steps:   3%|‚ñã                        | 282/10000 [04:24<21:18,  7.60it/s]Update steps:   3%|‚ñã                        | 283/10000 [04:24<21:17,  7.61it/s]Update steps:   3%|‚ñã                        | 284/10000 [04:24<21:18,  7.60it/s]Update steps:   3%|‚ñã                        | 285/10000 [04:24<21:17,  7.60it/s]Update steps:   3%|‚ñã                        | 286/10000 [04:24<21:18,  7.60it/s]Update steps:   3%|‚ñã                        | 287/10000 [04:24<21:17,  7.60it/s]Update steps:   3%|‚ñã                        | 288/10000 [04:25<21:17,  7.60it/s]Update steps:   3%|‚ñã                        | 289/10000 [04:25<21:16,  7.61it/s]Update steps:   3%|‚ñã                        | 290/10000 [04:25<21:16,  7.61it/s]Update steps:   3%|‚ñã                        | 291/10000 [04:25<21:16,  7.60it/s]Update steps:   3%|‚ñã                        | 292/10000 [04:25<21:16,  7.61it/s]Update steps:   3%|‚ñã                        | 293/10000 [04:25<21:16,  7.60it/s]Update steps:   3%|‚ñã                        | 294/10000 [04:25<21:16,  7.60it/s]Update steps:   3%|‚ñã                        | 295/10000 [04:26<21:17,  7.60it/s]Update steps:   3%|‚ñã                        | 296/10000 [04:26<21:16,  7.60it/s]Update steps:   3%|‚ñã                        | 297/10000 [04:26<21:15,  7.61it/s]Update steps:   3%|‚ñã                        | 298/10000 [04:26<21:17,  7.60it/s]Update steps:   3%|‚ñã                        | 299/10000 [04:26<21:17,  7.59it/s]Update steps:   3%|‚ñä                        | 300/10000 [04:26<21:16,  7.60it/s]W0501 16:51:14.699000 184101 torch/distributed/elastic/agent/server/api.py:719] Received Signals.SIGTERM death signal, shutting down workers
W0501 16:51:14.699000 184101 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 184189 closing signal SIGTERM
W0501 16:51:14.700000 184101 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 184190 closing signal SIGTERM
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 918, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 909, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 260, in launch_agent
    result = agent.run()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/metrics/api.py", line 137, in wrapper
    result = f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/agent/server/api.py", line 711, in run
    result = self._invoke_run(role)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/agent/server/api.py", line 870, in _invoke_run
    time.sleep(monitor_interval)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/api.py", line 84, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 184101 got signal: 15
